#############################
# 共通設定
#############################
# プロキシ設定
HTTP_PROXY="<プロキシ設定>"
http_proxy="<プロキシ設定>"
HTTPS_PROXY="<プロキシ設定>"
https_proxy="<プロキシ設定>"
NO_PROXY="<プロキシ設定>"
no_proxy="<プロキシ設定>"

# OpenAI APIキー
OPENAI_API_KEY="<Open AI APIキー>"

# Hugging Face アクセスキー (ローカルLLM用, vLLMサーバで利用)
HUGGING_FACE_HUB_TOKEN="<Hugging Face アクセストークン>"

#############################
# Tsuzumi用FastChatサーバ設定
#############################
# APIキー(カンマ区切りで複数指定可能、省略時空文字列指定時は認証なし)
APP_API_KEYS="8859b0cb,8859b0aa"

# ログ出力先
LOG_DIR=./backend/llm_containers/tsuzumi7B-v1.2/logs
# ログレベル(controlle)
LOG_LEVEL_CONTROLLER=INFO
# ログレベル(model_worker)
LOG_LEVEL_MODEL_WORKER=INFO
# ログレベル(openai_worker)
LOG_LEVEL_OPENAI_WORKER=INFO
# ログレベル(open_api_server)
LOG_LEVEL_OPEN_API_SERVER=INFO
# 入出力データのログ出力有効化フラグ
#  - True: リクエストボディ、レスポンスボディをログに出力する
#  - False: リクエストボディ、レスポンスボディをログに出力しない
ENABLE_INOUT_LOG=False

###model-worker1設定
# モデル名
WORKER1_MODEL_NAME=tsuzumi-7b-v1_2-8k-instruct
# モデルディレクトリパス(dockerコンテナ内)
WORKER1_MODEL_PATH=/model/tsuzumi-7b-v1_2-8k-instruct
# モデル種別(HF形式:"tsuzumi-hf")
WORKER1_MODEL_TYPE="tsuzumi-hf"
# 利用GPU ID
WORKER1_GPU_IDS=0
# DTYPE(HF形式:dtypeとして利用)
WORKER1_DTYPE=bfloat16
# システムプロンプト
WORKER1_SYSTEM_PROMPT="あなたは有益なAIアシスタントです。質問や指示を良く読んで従ってください。様々な観点からできる限り詳細に説明してください。\n"
# flash attention2指定(有効:"flash"、無効:"torch")
WORKER1_ATTN_IMPL="flash"
